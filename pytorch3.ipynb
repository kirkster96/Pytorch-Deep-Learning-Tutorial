{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will build the neural network and how to pass data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('', train = True, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ]))\n",
    "\n",
    "test = datasets.MNIST('', train = False, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor()\n",
    "                      ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn is more of the OOP version and torch.nn.functional is more functions. In general we will use OOP pytorch. There will be times when you want to run a specific function, but these two libraries are basically interchangable.\n",
    "\n",
    "Just keep in mind that with functional you will actually have to pass parameters with each function call. With OOP you can just initialize things.\n",
    "\n",
    "Most code ends up using both OOP and functional. You could write everything as a function in theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build our neural network!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): #create class called Net and inherit nn.Module\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()#super() corresponds to nn.Module initialization method running \n",
    "        '''Define layers fully connected (fc). nn.Linear(input output)\n",
    "        1st layer input is literally the images pixels dataset flattened.\n",
    "        In this case it is 784 pixels... 28x28, the image must be flattened.\n",
    "        The output is based on the target, in this case it is to have 3 layers\n",
    "        containing 64 neurons. The ouput and target # of neurons can be anything.'''\n",
    "        self.fc1 = nn.Linear(28*28, 64) #fully connected 1st layer. input layer.\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10) #output layer. 10 neurons for 10 choices (0-9).\n",
    "    \n",
    "    def forward(self, x): #this will be the method that defines how data flows\n",
    "        x = F.relu(self.fc1(x)) #rectified linear activation function\n",
    "        x = F.relu(self.fc2(x)) #rectified linear activation function\n",
    "        \n",
    "        '''fancy things can be done here as additional logic. \n",
    "        In order to switch between layers for a specific task at hand'''\n",
    "        \n",
    "        x = F.relu(self.fc3(x)) #rectified linear activation function\n",
    "        x = self.fc4(x)         \n",
    "        return F.log_softmax(x,dim=1) #output layer goal needs to pick one neuron.\n",
    "        \n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets pass in some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3427, -2.3449, -2.3239, -2.2798, -2.3007, -2.3084, -2.3934, -2.3173,\n",
       "         -2.2053, -2.2238]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1,28*28)\n",
    "\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
